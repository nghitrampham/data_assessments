{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cleaned_data[\"y\"] = df['y']\n",
    "\n",
    "# target\n",
    "y = cleaned_data['y']\n",
    "\n",
    "# features\n",
    "cleaned_data.drop(['y'], axis = 1, inplace = True)\n",
    "X = cleaned_data\n",
    "\n",
    "#Instantiate CatBoostClassifier\n",
    "cbc = CatBoostClassifier(class_weights={0:1, 1:6})\n",
    "\n",
    "# # Creating the hyperparameter grid\n",
    "# param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\n",
    "#                \"max_depth\": randint(3, 10)}\n",
    "# #Instantiate RandomSearchCV object\n",
    "# rscv = RandomizedSearchCV(cbc , param_dist, scoring='accuracy', cv =5)\n",
    "# {'learning_rate': 0.5, 'max_depth': 6, 'n_estimators': 1000}\n",
    "\n",
    "grid = {'max_depth': [3,4,5,6], 'n_estimators':[200, 500, 1000], \"learning_rate\": [0.01, 0.5, 1, 1.5]}   \n",
    "#Instantiate GridSearchCV\n",
    "gscv = GridSearchCV(estimator = cbc, param_grid = grid, scoring ='accuracy', cv = 5)\n",
    "\n",
    "\n",
    "#Fit the model\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "gscv.fit(X,y,cat_features=categorical_features_indices, plot = True)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(gscv.best_params_)\n",
    "print(gscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0743f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporting(ensem_preds, targets):\n",
    "    best_th = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for th in np.arange(0.0, 0.6, 0.01):\n",
    "        pred = (ensem_preds > th).astype(int)\n",
    "        score = f1_score(targets, pred)\n",
    "        if score > best_score:\n",
    "            best_th = th\n",
    "            best_score = score\n",
    "\n",
    "    print(f\"\\nAUC score: {roc_auc_score(targets, ensem_preds):12.4f}\")\n",
    "    print(f\"Best threshold {best_th:12.4f}\")\n",
    "\n",
    "    preds = (ensem_preds > best_th).astype(int)\n",
    "    # print(classification_report(targets, preds, digits=3))\n",
    "\n",
    "    cm1 = confusion_matrix(targets, preds)\n",
    "    print('\\nConfusion Matrix : \\n', cm1)\n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    print('\\n=============')\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print (f'Accuracy    : {accuracy1:12.4f}')\n",
    "\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print(f'Sensitivity : {sensitivity1:12.4f}')\n",
    "\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print(f'Specificity : {specificity1:12.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1796d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "class Model(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"mixed of lgb, xgb, et\"\"\"\n",
    "\n",
    "    def __init__(self, seed=0, nest_lgb=1.0, nest_xgb=1.0, nest_et=1.0, cbt=0.75, ss=0.75, alpha=0.5, pos_scale=1.):\n",
    "        \"\"\"\n",
    "        Top 3 tree models\n",
    "        \"\"\"\n",
    "        self.classes_ = [0,1]\n",
    "        self.models = [\n",
    "                         lgb.LGBMClassifier(class_weight='balanced', \n",
    "                                            num_leaves=31, \n",
    "                                            max_depth=-1, \n",
    "                                            min_child_samples=2, \n",
    "                                            learning_rate=0.02,\n",
    "                                            n_estimators=int(100*nest_lgb), \n",
    "                                            colsample_bytree=cbt, \n",
    "                                            subsample=ss, \n",
    "                                            n_jobs=-1, \n",
    "                                            random_state=0+seed\n",
    "                                           ),\n",
    "                         lgb.LGBMClassifier(class_weight=None, \n",
    "                                            num_leaves=31, \n",
    "                                            max_depth=-1, \n",
    "                                            min_child_samples=2, \n",
    "                                            learning_rate=0.01,\n",
    "                                            n_estimators=int(200*nest_lgb), \n",
    "                                            colsample_bytree=cbt, \n",
    "                                            subsample=ss, \n",
    "                                            n_jobs=-1, \n",
    "                                            random_state=1+seed\n",
    "                                           ),\n",
    "                         xgb.XGBClassifier(max_depth=12,\n",
    "                                           scale_pos_weight=1.,\n",
    "                                           learning_rate=0.01, \n",
    "                                           n_estimators=int(100 * nest_xgb),\n",
    "                                           subsample=ss, # 0.75\n",
    "                                           colsample_bytree=cbt, # 0.75\n",
    "                                           nthread=-1,\n",
    "                                           seed=0+seed,\n",
    "                                           eval_metric='logloss',\n",
    "                                           use_label_encoder=False\n",
    "                                          ),\n",
    "                         xgb.XGBClassifier(max_depth=6,\n",
    "                                           scale_pos_weight=pos_scale,\n",
    "                                           learning_rate=0.01,\n",
    "                                           n_estimators=int(200 * nest_xgb),\n",
    "                                           subsample=ss, # 0.75\n",
    "                                           colsample_bytree=cbt, # 0.75\n",
    "                                           nthread=-1,\n",
    "                                           seed=1+seed,\n",
    "                                           eval_metric='logloss',\n",
    "                                           use_label_encoder=False\n",
    "                                          ),\n",
    "                         ExtraTreesClassifier(class_weight='balanced', \n",
    "                                              bootstrap=False, \n",
    "                                              criterion='entropy', \n",
    "                                              max_features=cbt, \n",
    "                                              min_samples_leaf=4, \n",
    "                                              min_samples_split=3, \n",
    "                                              n_estimators= int(100 * nest_et), \n",
    "                                              random_state=0+seed, \n",
    "                                              n_jobs=-1),\n",
    "                         \n",
    "                      ]\n",
    "        self.weights = [(1-alpha)*1, (1-alpha)*1, (1-alpha)*1, (1-alpha)*1, (1-alpha)*1]\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for t, clf in enumerate(self.models):\n",
    "            clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        suma = 0.0\n",
    "        for t, clf in enumerate(self.models):\n",
    "            a = clf.predict_proba(X)[:, 1]\n",
    "            suma += (self.weights[t] * a)\n",
    "        return (suma / sum(self.weights))\n",
    "            \n",
    "    def predict_proba(self, X):      \n",
    "        return (self.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
